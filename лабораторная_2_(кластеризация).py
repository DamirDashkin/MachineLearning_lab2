# -*- coding: utf-8 -*-
"""Лабораторная 2 (Кластеризация) 01.12.2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rMk-ji9gdAKDX5sFXOjhZA9NaDtaMrPP
"""

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import numpy as np
from sklearn import metrics

# Загружаем данные и обрабатываем их
ds = pd.read_csv('Spotify_Youtube.csv')
url_cols = ['Url_spotify', 'Uri', 'Url_youtube', 'Title', 'Description', 'Unnamed: 0']
ds.drop(url_cols, axis=1, inplace=True)
ds.dropna(inplace=True)
ds = ds.drop_duplicates()

ds.head(10)

columns=['Artist','Track', 'Album', 'Album_type', 'Key', 'Channel', 'Comments','Licensed', 'official_video']
ds.drop(columns, axis=1, inplace=True)

ds.describe()

ML_DF = ds[['Danceability', 'Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence',
              'Tempo', 'Duration_ms', 'Views', 'Likes', 'Stream']]
#СТАНДАРТИЗАЦИЯ
scaler =StandardScaler()
features =scaler.fit_transform(ds)
scale_ds =pd.DataFrame(features,columns=['Danceability', 'Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence',
              'Tempo', 'Duration_ms', 'Views', 'Likes', 'Stream'])

#Нормализация
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaled_ds = scaler.fit_transform(scale_ds)
scaled_ds =pd.DataFrame(scaled_ds,columns=['Danceability', 'Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence',
              'Tempo', 'Duration_ms', 'Views', 'Likes', 'Stream'])

scale_ds.describe()

scaled_ds.describe()

"""Так как у нас в датасете множество параметров, которые учитываются при кластеризации, было решено взять PCA-алгоритм. Выбор количества компонентов зависит от совокупной накопленной дисперсии, которая по правилам должна быть больше, чем 0.95"""

#PCA-model
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(scaled_ds);
evr = pca.explained_variance_ratio_
evr

fig = plt.figure(figsize=(10,8))
plt.plot(range(1, len(scaled_ds.columns)+1), evr.cumsum(), marker='o', linestyle='--')
plt.xlabel('Number of Components', fontsize=18)
plt.ylabel('Cumulative Explained Variance',fontsize=18)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.show()

for i, exp_var in enumerate(evr.cumsum()):
    if exp_var >= 0.95:
        n_comps = i + 1
        break
print("Number of components:", n_comps)

pca = PCA(n_components = n_comps)
scale_ds_PCA = pca.fit_transform(scaled_ds);
scale_ds_PCA = pd.DataFrame(scale_ds_PCA, index=scaled_ds.index, columns=['PC1','PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8'])
scale_ds_PCA

"""#Метод k-средних"""

#МЕТОД ЛОКТЯ ДЛЯ ОПРЕДЕЛЕНИЯ ОПТИМАЛЬНОГО K

wcss = {}
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'random', random_state = 44)
    kmeans.fit(scale_ds_PCA.values)
    wcss[i] = kmeans.inertia_

plt.plot(wcss.keys(), wcss.values(), 'gs-')
plt.xlabel("k")
plt.ylabel('WCSS')
plt.show()

"""Я выберу k = 3"""

k = 3

Km = KMeans(init='random', n_clusters=k, n_init=50)

CLUSTERS = Km.fit_predict(scale_ds_PCA.values)

print(f"Silhouette Coefficient: {metrics.silhouette_score(scale_ds_PCA.values, CLUSTERS):.3f}")

print(f"Calinski_Harabasz_Score: {metrics.calinski_harabasz_score(scale_ds_PCA.values, CLUSTERS):.3f}")

print(f"Davies_Bouldin_Score: {metrics.davies_bouldin_score(scale_ds_PCA.values, CLUSTERS):.3f}")

scale_ds_PCA['Cluster'] = CLUSTERS
scale_ds_PCA['Cluster'].astype('category')
#Визуализация
plt.scatter(scale_ds_PCA['PC1'], scale_ds_PCA['PC2'], c=scale_ds_PCA['Cluster'], cmap='viridis')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('K-means Clustering')
plt.show()

#Визуализация (более красивая)

import plotly.express as px

fig = px.scatter(scale_ds_PCA, x=scale_ds_PCA['PC1'], y=scale_ds_PCA['PC2'], color='Cluster', width=900, height=600,
                color_discrete_sequence=px.colors.sequential.Aggrnyl_r)
fig.update_traces(marker=dict(opacity=1, line=dict(width=0.2,color='black') ) )


# Centroids_DF = pd.DataFrame(dict(
#     x = [Centroids[0,0], Centroids[1,0], Centroids[2,0]],
#     y = [Centroids[0,1], Centroids[1,1], Centroids[2,1]]    ))

# trace = px.scatter(Centroids_DF, x='x', y='y', marginal_x='histogram', marginal_y='histogram')
# fig.add_trace(trace.data[0])

# fig.update_traces(marker=dict(symbol='x', color='red', size = 25, opacity=1),
#                   selector=dict(type='scatter', uid=trace.data[0].uid))

fig.update_yaxes(title_text='PC2')
fig.update_xaxes(title_text='PC1')
fig.update_layout(title=dict(text='<b>Clusters in PC Space (for interactivity)</b>', x=0.51, y=0.90,
                font=dict( family="Helvetica", size=26) ), title_font_color='black', margin=dict(t=100))
fig.show()

scale_ds_PCA.drop('Cluster', axis=1, inplace=True)

"""#DBSCAN"""

from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
scale_ds_PCA

#scale_ds_PCA.drop(columns=['PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8'])
m2 = scale_ds_PCA.values
knn = NearestNeighbors(n_neighbors = 16)
model = knn.fit(m2)
distances, indices = knn.kneighbors(m2)
distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.xlabel('Points Sorted by Distance')
plt.ylabel('16-NN Distance')
plt.title('K-Distance Graph');
plt.grid()
plt.plot(distances);

db = DBSCAN(eps = 0.18, min_samples = 17).fit(m2)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

# Number of clusters in labels, ignoring noise if present

n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)
print('Number of Clusters : ', n_clusters_)
print('Number of Outliers : ', n_noise_)

unique_labels = set(labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 0]
    class_member_mask = labels == k

    xy = m2[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], "o", markerfacecolor = tuple(col), markeredgecolor = "k", markersize = 8)

    xy = m2[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], "o", markerfacecolor = tuple(col), markeredgecolor = "k", markersize = 2)

plt.title("Estimated number of clusters: %d" % n_clusters_)
plt.show()

"""#Hierarchy"""

from scipy.cluster.hierarchy import dendrogram, linkage

linkage_data = linkage(m2, method = 'ward', metric = 'euclidean')
dendrogram(linkage_data)
plt.tight_layout()
plt.axhline(y=32, c='k', ls='--', lw=0.5)
plt.show()

"""Я выберу 3 кластера, отсеку дендограмму на расстоянии 32"""

from scipy.cluster.hierarchy import fcluster

#по дендрограмме выбираем количество кластеров
k = 3

cluster_labels = fcluster(Z, k, criterion='maxclust')
scale_ds_PCA['Cluster_dendro'] = cluster_labels

scale_ds_PCA['Cluster_dendro'].describe()

print(scale_ds_PCA.head())
plt.figure(figsize=(10, 8))


for cluster in range(1, k+1):
    cluster_subset = scale_ds_PCA[scale_ds_PCA['Cluster_dendro'] == cluster]
    plt.scatter(cluster_subset['PC1'], cluster_subset['PC2'], label=f'Кластер {cluster}')

plt.title('Иерархическая кластеризация ')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

from sklearn.cluster import AgglomerativeClustering
hierarchical_cluster = AgglomerativeClustering(n_clusters = k, metric = 'euclidean', linkage = 'ward')
Labels = hierarchical_cluster.fit(m2)
H_labels = Labels.labels_
pred_agc = pd.Series(hierarchical_cluster.labels_)
scale_ds_PCA['Class'] = pred_agc

scale_ds_PCA['Class'].describe()

print(scale_ds_PCA.head())
plt.figure(figsize=(10, 8))


for cluster in range(0, k):
    cluster_subset = scale_ds_PCA[scale_ds_PCA['Class'] == cluster]
    plt.scatter(cluster_subset['PC1'], cluster_subset['PC2'], label=f'Кластер {cluster}')

plt.title('Иерархическая кластеризация ')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

"""# Оценка производительности кластеризации"""

print(scale_ds_PCA)
columns = ['Class', 'Cluster_dendro']
#scale_ds_PCA.drop(columns, axis=1, inplace=True)
print(scale_ds_PCA)
m2 = scale_ds_PCA.values

print("-------------------------------------------K-means-----------------------------------------------")

print(f"Silhouette Coefficient: {metrics.silhouette_score(m2, CLUSTERS):.3f}")

print(f"Calinski_Harabasz_Score: {metrics.calinski_harabasz_score(m2, CLUSTERS):.3f}")

print(f"Davies_Bouldin_Score: {metrics.davies_bouldin_score(m2, CLUSTERS):.3f}")


print("-------------------------------------------DBSCAN------------------------------------------------")

print(f"Silhouette Coefficient: {metrics.silhouette_score(m2, labels):.3f}")

print(f"Calinski_Harabasz_Score: {metrics.calinski_harabasz_score(m2, labels):.3f}")

print(f"Davies_Bouldin_Score: {metrics.davies_bouldin_score(m2, labels):.3f}")


print("------------------------------------------Hierarchy----------------------------------------------")

print(f"Silhouette Coefficient: {metrics.silhouette_score(m2, hierarchical_cluster.labels_):.3f}")

print(f"Calinski_Harabasz_Score: {metrics.calinski_harabasz_score(m2, hierarchical_cluster.labels_):.3f}")

print(f"Davies_Bouldin_Score: {metrics.davies_bouldin_score(m2, hierarchical_cluster.labels_):.3f}")